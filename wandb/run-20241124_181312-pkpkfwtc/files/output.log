model.blocks.0.attn에 dropout 삽입! : class = Attention(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (q_norm): Identity()
  (k_norm): Identity()
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
)
model.blocks.0.mlp에 dropout 삽입! : class = Mlp(
  (fc1): Linear(in_features=768, out_features=3072, bias=True)
  (act): GELU(approximate='none')
  (drop1): Dropout(p=0.0, inplace=False)
  (norm): Identity()
  (fc2): Linear(in_features=3072, out_features=768, bias=True)
  (drop2): Dropout(p=0.0, inplace=False)
)
model.blocks.1.attn에 dropout 삽입! : class = Attention(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (q_norm): Identity()
  (k_norm): Identity()
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
)
model.blocks.1.mlp에 dropout 삽입! : class = Mlp(
  (fc1): Linear(in_features=768, out_features=3072, bias=True)
  (act): GELU(approximate='none')
  (drop1): Dropout(p=0.0, inplace=False)
  (norm): Identity()
  (fc2): Linear(in_features=3072, out_features=768, bias=True)
  (drop2): Dropout(p=0.0, inplace=False)
)
model.blocks.2.attn에 dropout 삽입! : class = Attention(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (q_norm): Identity()
  (k_norm): Identity()
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
)
model.blocks.2.mlp에 dropout 삽입! : class = Mlp(
  (fc1): Linear(in_features=768, out_features=3072, bias=True)
  (act): GELU(approximate='none')
  (drop1): Dropout(p=0.0, inplace=False)
  (norm): Identity()
  (fc2): Linear(in_features=3072, out_features=768, bias=True)
  (drop2): Dropout(p=0.0, inplace=False)
)
model.blocks.3.attn에 dropout 삽입! : class = Attention(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (q_norm): Identity()
  (k_norm): Identity()
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
)
model.blocks.3.mlp에 dropout 삽입! : class = Mlp(
  (fc1): Linear(in_features=768, out_features=3072, bias=True)
  (act): GELU(approximate='none')
  (drop1): Dropout(p=0.0, inplace=False)
  (norm): Identity()
  (fc2): Linear(in_features=3072, out_features=768, bias=True)
  (drop2): Dropout(p=0.0, inplace=False)
)
model.blocks.4.attn에 dropout 삽입! : class = Attention(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (q_norm): Identity()
  (k_norm): Identity()
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
)
model.blocks.4.mlp에 dropout 삽입! : class = Mlp(
  (fc1): Linear(in_features=768, out_features=3072, bias=True)
  (act): GELU(approximate='none')
  (drop1): Dropout(p=0.0, inplace=False)
  (norm): Identity()
  (fc2): Linear(in_features=3072, out_features=768, bias=True)
  (drop2): Dropout(p=0.0, inplace=False)
)
model.blocks.5.attn에 dropout 삽입! : class = Attention(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (q_norm): Identity()
  (k_norm): Identity()
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
)
model.blocks.5.mlp에 dropout 삽입! : class = Mlp(
  (fc1): Linear(in_features=768, out_features=3072, bias=True)
  (act): GELU(approximate='none')
  (drop1): Dropout(p=0.0, inplace=False)
  (norm): Identity()
  (fc2): Linear(in_features=3072, out_features=768, bias=True)
  (drop2): Dropout(p=0.0, inplace=False)
)
model.blocks.6.attn에 dropout 삽입! : class = Attention(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (q_norm): Identity()
  (k_norm): Identity()
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
)
model.blocks.6.mlp에 dropout 삽입! : class = Mlp(
  (fc1): Linear(in_features=768, out_features=3072, bias=True)
  (act): GELU(approximate='none')
  (drop1): Dropout(p=0.0, inplace=False)
  (norm): Identity()
  (fc2): Linear(in_features=3072, out_features=768, bias=True)
  (drop2): Dropout(p=0.0, inplace=False)
)
model.blocks.7.attn에 dropout 삽입! : class = Attention(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (q_norm): Identity()
  (k_norm): Identity()
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
)
model.blocks.7.mlp에 dropout 삽입! : class = Mlp(
  (fc1): Linear(in_features=768, out_features=3072, bias=True)
  (act): GELU(approximate='none')
  (drop1): Dropout(p=0.0, inplace=False)
  (norm): Identity()
  (fc2): Linear(in_features=3072, out_features=768, bias=True)
  (drop2): Dropout(p=0.0, inplace=False)
)
model.blocks.8.attn에 dropout 삽입! : class = Attention(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (q_norm): Identity()
  (k_norm): Identity()
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
)
model.blocks.8.mlp에 dropout 삽입! : class = Mlp(
  (fc1): Linear(in_features=768, out_features=3072, bias=True)
  (act): GELU(approximate='none')
  (drop1): Dropout(p=0.0, inplace=False)
  (norm): Identity()
  (fc2): Linear(in_features=3072, out_features=768, bias=True)
  (drop2): Dropout(p=0.0, inplace=False)
)
model.blocks.9.attn에 dropout 삽입! : class = Attention(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (q_norm): Identity()
  (k_norm): Identity()
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
)
model.blocks.9.mlp에 dropout 삽입! : class = Mlp(
  (fc1): Linear(in_features=768, out_features=3072, bias=True)
  (act): GELU(approximate='none')
  (drop1): Dropout(p=0.0, inplace=False)
  (norm): Identity()
  (fc2): Linear(in_features=3072, out_features=768, bias=True)
  (drop2): Dropout(p=0.0, inplace=False)
)
model.blocks.10.attn에 dropout 삽입! : class = Attention(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (q_norm): Identity()
  (k_norm): Identity()
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
)
model.blocks.10.mlp에 dropout 삽입! : class = Mlp(
  (fc1): Linear(in_features=768, out_features=3072, bias=True)
  (act): GELU(approximate='none')
  (drop1): Dropout(p=0.0, inplace=False)
  (norm): Identity()
  (fc2): Linear(in_features=3072, out_features=768, bias=True)
  (drop2): Dropout(p=0.0, inplace=False)
)
model.blocks.11.attn에 dropout 삽입! : class = Attention(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (q_norm): Identity()
  (k_norm): Identity()
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
)
model.blocks.11.mlp에 dropout 삽입! : class = Mlp(
  (fc1): Linear(in_features=768, out_features=3072, bias=True)
  (act): GELU(approximate='none')
  (drop1): Dropout(p=0.0, inplace=False)
  (norm): Identity()
  (fc2): Linear(in_features=3072, out_features=768, bias=True)
  (drop2): Dropout(p=0.0, inplace=False)
)
  0%|                                                                                                              | 0/3125 [00:00<?, ?it/s]
> /home/bscho/works/multi-tta/models/baseline.py(72)forward()
-> x = self.patch_embed(samples)  # 패치 임베딩
*** AttributeError: 'Tensor' object has no attribute 'keys'
tensor([[[[ 2.2489,  2.2489,  1.8893,  ...,  0.8789,  2.2147,  1.1187],
          [ 1.0502,  0.9474,  0.4166,  ...,  0.7077,  1.6667,  1.2557],
          [ 2.2489, -0.4226, -0.6794,  ...,  1.0331,  1.1187, -0.3027],
          ...,
          [ 0.8276,  0.6734,  1.1700,  ..., -0.1314, -1.2959,  0.1426],
          [ 1.0502, -0.9877,  0.1426,  ...,  0.6734,  1.1358,  2.2489],
          [ 1.4098,  1.6838,  0.7077,  ..., -1.0562,  0.0569,  0.9132]],

         [[ 2.2535,  2.0784,  1.4132,  ...,  0.3452,  1.9209,  0.9230],
          [ 0.8179,  0.8004,  0.3102,  ...,  0.4328,  1.4307,  1.0105],
          [ 2.1134, -0.3375,  0.0126,  ...,  1.2906,  1.0280, -0.5826],
          ...,
          [ 1.3957,  1.2731,  1.7633,  ...,  0.3277, -1.2304,  0.0826],
          [ 0.5203, -1.0028,  1.1681,  ...,  1.0280,  0.6429,  1.6933],
          [ 0.3277,  1.4132,  2.0084,  ..., -0.8102, -0.7752, -0.4776]],

         [[ 2.6400,  2.4483,  1.5245,  ...,  1.5245,  2.6400,  2.0648],
          [ 1.4374,  1.3328,  0.7925,  ...,  1.2108,  2.1346,  1.6814],
          [ 2.6400,  0.5834,  1.1237,  ...,  1.2631,  0.7925, -0.8981],
          ...,
          [ 1.1934,  1.2980,  2.3263,  ...,  0.6705, -1.6824, -0.8110],
          [ 1.5071, -0.1835,  1.6814,  ...,  1.6988,  0.5834,  1.2631],
          [ 1.8731,  2.6226,  2.5006,  ...,  0.0431, -0.6018, -0.6367]]],


        [[[-0.9192, -0.5938, -0.6794,  ..., -2.1179, -0.0116, -0.8335],
          [-1.2445, -1.0733, -1.6727,  ..., -1.0904, -1.4672,  0.4166],
          [ 0.7762,  0.5878, -1.1932,  ..., -1.2445,  0.6221, -0.5424],
          ...,
          [ 0.1083, -2.1179, -1.2445,  ..., -0.5253, -0.4054, -1.7240],
          [-0.1657,  0.0398, -1.7412,  ..., -1.5014,  0.0741, -1.1075],
          [ 0.9474,  1.7352, -1.1247,  ..., -1.5185, -1.9638, -0.7479]],

         [[ 0.8704,  0.7654, -0.1099,  ..., -1.4930,  0.2577, -0.8102],
          [-0.3725, -0.3725, -1.3880,  ..., -0.4251, -0.9328,  0.9405],
          [-0.0399,  0.0126, -1.4230,  ..., -0.7052,  1.7983,  0.8880],
          ...,
          [ 0.9055, -1.7206, -0.1800,  ...,  0.0651,  0.3978, -0.8102],
          [-0.2150,  0.3627, -0.7227,  ..., -0.3200,  1.5357,  0.4153],
          [ 0.5378,  1.8333, -0.0749,  ..., -0.0224, -0.2675,  1.1155]],

         [[ 0.2871,  0.3916, -0.1138,  ..., -0.8458,  0.6705, -0.4798],
          [-0.4450, -0.3927, -1.2641,  ...,  0.1999, -0.6193,  1.1062],
          [ 0.9145,  0.7576, -1.0898,  ..., -0.0964,  1.9080,  0.7576],
          ...,
          [ 1.2805, -1.6127, -0.5147,  ...,  0.0256,  0.9145, -0.0615],
          [-0.9156, -0.4624, -1.6824,  ..., -0.7761,  1.5420,  0.6879],
          [-0.7238,  0.5311, -1.3513,  ..., -0.6890, -0.4798,  1.1237]]],


        [[[ 1.7865,  0.6392,  0.9988,  ..., -0.3027, -0.7308, -0.7650],
          [ 0.2624, -0.6794,  1.8208,  ..., -0.7479, -0.1486,  0.4508],
          [ 1.6667,  1.1358,  1.9578,  ..., -0.7308,  1.0844,  0.1939],
          ...,
          [-1.5014, -0.9020, -0.5767,  ...,  0.1597, -0.0116, -0.5938],
          [-1.5528, -2.1179, -1.6727,  ..., -0.5424, -0.4568, -0.4911],
          [-0.2342,  1.3070, -1.8439,  ..., -0.6281,  0.5364,  0.1083]],

         [[ 2.4286,  1.4832,  2.1485,  ..., -1.6331, -1.7031, -1.5455],
          [ 1.0455,  0.0651,  2.4286,  ..., -1.7556, -1.0378, -0.3725],
          [ 2.4286,  1.7808,  1.8859,  ..., -1.1078,  0.3627, -0.7752],
          ...,
          [-0.4601,  0.2927,  0.9755,  ..., -0.9678, -0.8803, -1.3004],
          [-1.0028, -1.9132, -1.1779,  ..., -1.0553, -1.3179, -1.5280],
          [ 0.1001,  1.5707, -1.8431,  ..., -0.8277, -0.3200, -1.0903]],

         [[ 2.6400,  1.7337,  2.0997,  ..., -1.0898, -0.6541, -0.2881],
          [ 1.3677,  0.2173,  2.4483,  ..., -1.0550,  0.0256,  0.8448],
          [ 2.6400,  1.7337,  1.6291,  ..., -0.0441,  1.4722,  0.3742],
          ...,
          [-1.6999, -0.9853, -0.5147,  ..., -1.1770, -0.9504, -1.3513],
          [-0.3055, -1.4907, -1.2641,  ..., -1.7347, -1.5604, -1.5779],
          [ 1.7511,  2.6400, -1.2293,  ..., -1.7522, -0.6193, -1.1073]]],


        ...,


        [[[ 1.8893,  0.3994,  0.8618,  ...,  1.8037,  1.4269,  0.9303],
          [ 1.6667,  2.2489,  2.2318,  ...,  0.4851,  1.5468,  1.2043],
          [ 1.7865,  2.1290,  1.9064,  ...,  2.2489,  1.2899,  2.2489],
          ...,
          [ 1.5810, -0.1143,  0.9132,  ...,  1.0844,  1.2043,  0.6221],
          [-0.3369,  1.5297,  1.2728,  ...,  0.5022,  0.9817,  1.1529],
          [ 1.3070,  0.2282, -0.2342,  ...,  2.2489,  2.2489,  1.0502]],

         [[ 0.1176, -0.7402,  0.9755,  ...,  2.4111,  2.2710,  1.9209],
          [ 0.5378,  1.5882,  2.2710,  ...,  0.8354,  2.0609,  1.7633],
          [ 1.9209,  2.1660,  1.6933,  ...,  2.2710,  1.0455,  2.2710],
          ...,
          [ 1.4307, -0.4251,  0.3102,  ...,  1.2381,  0.9930,  0.2402],
          [ 0.1702,  2.1835,  2.1835,  ...,  0.3627,  0.5903,  0.6429],
          [ 2.2010,  1.3782,  1.3431,  ...,  2.1835,  2.2185,  0.4853]],

         [[ 1.4897,  0.4439,  1.8731,  ...,  2.6400,  2.1520,  1.4374],
          [ 1.3677,  2.4657,  2.6400,  ...,  1.4722,  2.0823,  1.5245],
          [ 1.7337,  2.3437,  2.6400,  ...,  2.6400,  1.4374,  2.5180],
          ...,
          [ 1.4722, -0.2707,  0.7054,  ...,  0.8274,  0.7054, -0.0267],
          [-0.4101,  1.6988,  1.9254,  ...,  0.0082,  0.7576,  1.0888],
          [ 1.2980,  0.5659,  0.7925,  ...,  1.8557,  2.6051,  1.2631]]],


        [[[ 1.5468, -0.6965,  0.7419,  ...,  0.6221,  0.2282,  1.5810],
          [ 1.4612,  1.7694,  0.0569,  ...,  0.5707,  1.5297,  1.2043],
          [ 0.8104,  0.0227,  0.7591,  ...,  0.8789, -0.4739, -0.3198],
          ...,
          [ 0.1768, -0.5596, -1.0733,  ..., -0.7650,  1.3927,  0.1254],
          [-0.7650, -0.0801, -0.2513,  ..., -1.7412,  1.9749,  2.2489],
          [ 0.2967, -0.0458, -1.9124,  ..., -1.0219,  1.7180,  0.7762]],

         [[ 2.3235,  0.2577,  2.1660,  ...,  0.3978,  0.8880,  2.4286],
          [ 1.9384,  2.4286,  1.4307,  ...,  0.5728,  2.3585,  2.4286],
          [ 0.6254,  0.2927,  2.0084,  ...,  1.3431,  0.6078,  1.0630],
          ...,
          [-0.7927, -1.0203, -0.5126,  ..., -0.3725,  1.0105, -0.6702],
          [-1.8081, -0.5476,  0.3803,  ..., -1.8957,  1.2031,  1.9384],
          [-0.7227, -0.5126, -1.3004,  ..., -1.4055,  0.6954, -0.6001]],

         [[ 1.0714, -0.8110,  1.4200,  ...,  1.3677,  1.2980,  2.6400],
          [ 1.3154,  1.9777,  0.9494,  ...,  1.4374,  2.6400,  2.6051],
          [ 1.2282,  0.7402,  2.0823,  ...,  1.9428,  0.9494,  1.2631],
          ...,
          [-0.4275, -0.8807, -0.7587,  ..., -0.7761,  0.2348, -1.6650],
          [-1.8044, -0.7587, -0.1661,  ..., -1.6824,  0.6182,  0.9668],
          [-0.9504, -0.8981, -1.8044,  ..., -0.9330,  0.2348, -1.5430]]],


        [[[-2.1179, -0.7479, -2.1179,  ...,  0.2111,  0.7248, -1.1760],
          [-0.9363, -0.0972, -1.0048,  ..., -1.6384, -0.6109, -1.2274],
          [-1.5357, -2.0665, -0.7993,  ..., -1.6384, -0.1486, -0.3712],
          ...,
          [-0.7993,  0.3652, -0.8164,  ..., -1.2445, -0.8507, -0.8678],
          [-1.1247, -0.0801, -0.0287,  ..., -0.7137, -0.2856, -0.2171],
          [-1.5357, -1.4329, -2.1179,  ..., -1.7583, -2.1179, -1.8097]],

         [[-0.4251,  1.7983, -0.6877,  ...,  1.5182,  2.4286,  1.1506],
          [ 1.3782,  2.0959,  0.9055,  ..., -0.2325,  1.4482,  1.1681],
          [-0.0049, -0.6352,  0.4678,  ...,  0.0826,  2.1134,  2.1310],
          ...,
          [ 1.3256,  2.1310,  0.2402,  ..., -1.1253, -0.9153, -1.0028],
          [ 1.0805,  1.9559,  1.6057,  ...,  0.7654,  1.2381,  1.3081],
          [ 0.7304,  0.7129, -0.8277,  ...,  0.3978, -0.1800,  0.5553]],

         [[-1.8044,  0.5659, -1.0376,  ...,  2.0300,  1.8208, -0.4973],
          [ 0.1302,  1.2980,  1.0539,  ...,  0.0953,  0.4788, -0.4275],
          [-0.4798, -0.5844,  1.5942,  ..., -0.0092,  1.0539,  0.6008],
          ...,
          [ 1.2805,  2.0125, -0.0964,  ..., -1.1770,  0.5485,  1.2108],
          [ 0.8099,  1.8383,  1.8034,  ...,  1.6117,  2.0648,  2.0997],
          [ 0.3219,  0.6008, -0.3404,  ...,  1.6988,  0.3045,  0.6356]]]],
       device='cuda:0')
*** AttributeError: 'Tensor' object has no attribute 'keys'
tensor([[[[ 2.2489,  2.2489,  1.8893,  ...,  0.8789,  2.2147,  1.1187],
          [ 1.0502,  0.9474,  0.4166,  ...,  0.7077,  1.6667,  1.2557],
          [ 2.2489, -0.4226, -0.6794,  ...,  1.0331,  1.1187, -0.3027],
          ...,
          [ 0.8276,  0.6734,  1.1700,  ..., -0.1314, -1.2959,  0.1426],
          [ 1.0502, -0.9877,  0.1426,  ...,  0.6734,  1.1358,  2.2489],
          [ 1.4098,  1.6838,  0.7077,  ..., -1.0562,  0.0569,  0.9132]],

         [[ 2.2535,  2.0784,  1.4132,  ...,  0.3452,  1.9209,  0.9230],
          [ 0.8179,  0.8004,  0.3102,  ...,  0.4328,  1.4307,  1.0105],
          [ 2.1134, -0.3375,  0.0126,  ...,  1.2906,  1.0280, -0.5826],
          ...,
          [ 1.3957,  1.2731,  1.7633,  ...,  0.3277, -1.2304,  0.0826],
          [ 0.5203, -1.0028,  1.1681,  ...,  1.0280,  0.6429,  1.6933],
          [ 0.3277,  1.4132,  2.0084,  ..., -0.8102, -0.7752, -0.4776]],

         [[ 2.6400,  2.4483,  1.5245,  ...,  1.5245,  2.6400,  2.0648],
          [ 1.4374,  1.3328,  0.7925,  ...,  1.2108,  2.1346,  1.6814],
          [ 2.6400,  0.5834,  1.1237,  ...,  1.2631,  0.7925, -0.8981],
          ...,
          [ 1.1934,  1.2980,  2.3263,  ...,  0.6705, -1.6824, -0.8110],
          [ 1.5071, -0.1835,  1.6814,  ...,  1.6988,  0.5834,  1.2631],
          [ 1.8731,  2.6226,  2.5006,  ...,  0.0431, -0.6018, -0.6367]]],


        [[[-0.9192, -0.5938, -0.6794,  ..., -2.1179, -0.0116, -0.8335],
          [-1.2445, -1.0733, -1.6727,  ..., -1.0904, -1.4672,  0.4166],
          [ 0.7762,  0.5878, -1.1932,  ..., -1.2445,  0.6221, -0.5424],
          ...,
          [ 0.1083, -2.1179, -1.2445,  ..., -0.5253, -0.4054, -1.7240],
          [-0.1657,  0.0398, -1.7412,  ..., -1.5014,  0.0741, -1.1075],
          [ 0.9474,  1.7352, -1.1247,  ..., -1.5185, -1.9638, -0.7479]],

         [[ 0.8704,  0.7654, -0.1099,  ..., -1.4930,  0.2577, -0.8102],
          [-0.3725, -0.3725, -1.3880,  ..., -0.4251, -0.9328,  0.9405],
          [-0.0399,  0.0126, -1.4230,  ..., -0.7052,  1.7983,  0.8880],
          ...,
          [ 0.9055, -1.7206, -0.1800,  ...,  0.0651,  0.3978, -0.8102],
          [-0.2150,  0.3627, -0.7227,  ..., -0.3200,  1.5357,  0.4153],
          [ 0.5378,  1.8333, -0.0749,  ..., -0.0224, -0.2675,  1.1155]],

         [[ 0.2871,  0.3916, -0.1138,  ..., -0.8458,  0.6705, -0.4798],
          [-0.4450, -0.3927, -1.2641,  ...,  0.1999, -0.6193,  1.1062],
          [ 0.9145,  0.7576, -1.0898,  ..., -0.0964,  1.9080,  0.7576],
          ...,
          [ 1.2805, -1.6127, -0.5147,  ...,  0.0256,  0.9145, -0.0615],
          [-0.9156, -0.4624, -1.6824,  ..., -0.7761,  1.5420,  0.6879],
          [-0.7238,  0.5311, -1.3513,  ..., -0.6890, -0.4798,  1.1237]]],


        [[[ 1.7865,  0.6392,  0.9988,  ..., -0.3027, -0.7308, -0.7650],
          [ 0.2624, -0.6794,  1.8208,  ..., -0.7479, -0.1486,  0.4508],
          [ 1.6667,  1.1358,  1.9578,  ..., -0.7308,  1.0844,  0.1939],
          ...,
          [-1.5014, -0.9020, -0.5767,  ...,  0.1597, -0.0116, -0.5938],
          [-1.5528, -2.1179, -1.6727,  ..., -0.5424, -0.4568, -0.4911],
          [-0.2342,  1.3070, -1.8439,  ..., -0.6281,  0.5364,  0.1083]],

         [[ 2.4286,  1.4832,  2.1485,  ..., -1.6331, -1.7031, -1.5455],
          [ 1.0455,  0.0651,  2.4286,  ..., -1.7556, -1.0378, -0.3725],
          [ 2.4286,  1.7808,  1.8859,  ..., -1.1078,  0.3627, -0.7752],
          ...,
          [-0.4601,  0.2927,  0.9755,  ..., -0.9678, -0.8803, -1.3004],
          [-1.0028, -1.9132, -1.1779,  ..., -1.0553, -1.3179, -1.5280],
          [ 0.1001,  1.5707, -1.8431,  ..., -0.8277, -0.3200, -1.0903]],

         [[ 2.6400,  1.7337,  2.0997,  ..., -1.0898, -0.6541, -0.2881],
          [ 1.3677,  0.2173,  2.4483,  ..., -1.0550,  0.0256,  0.8448],
          [ 2.6400,  1.7337,  1.6291,  ..., -0.0441,  1.4722,  0.3742],
          ...,
          [-1.6999, -0.9853, -0.5147,  ..., -1.1770, -0.9504, -1.3513],
          [-0.3055, -1.4907, -1.2641,  ..., -1.7347, -1.5604, -1.5779],
          [ 1.7511,  2.6400, -1.2293,  ..., -1.7522, -0.6193, -1.1073]]],


        ...,


        [[[ 1.8893,  0.3994,  0.8618,  ...,  1.8037,  1.4269,  0.9303],
          [ 1.6667,  2.2489,  2.2318,  ...,  0.4851,  1.5468,  1.2043],
          [ 1.7865,  2.1290,  1.9064,  ...,  2.2489,  1.2899,  2.2489],
          ...,
          [ 1.5810, -0.1143,  0.9132,  ...,  1.0844,  1.2043,  0.6221],
          [-0.3369,  1.5297,  1.2728,  ...,  0.5022,  0.9817,  1.1529],
          [ 1.3070,  0.2282, -0.2342,  ...,  2.2489,  2.2489,  1.0502]],

         [[ 0.1176, -0.7402,  0.9755,  ...,  2.4111,  2.2710,  1.9209],
          [ 0.5378,  1.5882,  2.2710,  ...,  0.8354,  2.0609,  1.7633],
          [ 1.9209,  2.1660,  1.6933,  ...,  2.2710,  1.0455,  2.2710],
          ...,
          [ 1.4307, -0.4251,  0.3102,  ...,  1.2381,  0.9930,  0.2402],
          [ 0.1702,  2.1835,  2.1835,  ...,  0.3627,  0.5903,  0.6429],
          [ 2.2010,  1.3782,  1.3431,  ...,  2.1835,  2.2185,  0.4853]],

         [[ 1.4897,  0.4439,  1.8731,  ...,  2.6400,  2.1520,  1.4374],
          [ 1.3677,  2.4657,  2.6400,  ...,  1.4722,  2.0823,  1.5245],
          [ 1.7337,  2.3437,  2.6400,  ...,  2.6400,  1.4374,  2.5180],
          ...,
          [ 1.4722, -0.2707,  0.7054,  ...,  0.8274,  0.7054, -0.0267],
          [-0.4101,  1.6988,  1.9254,  ...,  0.0082,  0.7576,  1.0888],
          [ 1.2980,  0.5659,  0.7925,  ...,  1.8557,  2.6051,  1.2631]]],


        [[[ 1.5468, -0.6965,  0.7419,  ...,  0.6221,  0.2282,  1.5810],
          [ 1.4612,  1.7694,  0.0569,  ...,  0.5707,  1.5297,  1.2043],
          [ 0.8104,  0.0227,  0.7591,  ...,  0.8789, -0.4739, -0.3198],
          ...,
          [ 0.1768, -0.5596, -1.0733,  ..., -0.7650,  1.3927,  0.1254],
          [-0.7650, -0.0801, -0.2513,  ..., -1.7412,  1.9749,  2.2489],
          [ 0.2967, -0.0458, -1.9124,  ..., -1.0219,  1.7180,  0.7762]],

         [[ 2.3235,  0.2577,  2.1660,  ...,  0.3978,  0.8880,  2.4286],
          [ 1.9384,  2.4286,  1.4307,  ...,  0.5728,  2.3585,  2.4286],
          [ 0.6254,  0.2927,  2.0084,  ...,  1.3431,  0.6078,  1.0630],
          ...,
          [-0.7927, -1.0203, -0.5126,  ..., -0.3725,  1.0105, -0.6702],
          [-1.8081, -0.5476,  0.3803,  ..., -1.8957,  1.2031,  1.9384],
          [-0.7227, -0.5126, -1.3004,  ..., -1.4055,  0.6954, -0.6001]],

         [[ 1.0714, -0.8110,  1.4200,  ...,  1.3677,  1.2980,  2.6400],
          [ 1.3154,  1.9777,  0.9494,  ...,  1.4374,  2.6400,  2.6051],
          [ 1.2282,  0.7402,  2.0823,  ...,  1.9428,  0.9494,  1.2631],
          ...,
          [-0.4275, -0.8807, -0.7587,  ..., -0.7761,  0.2348, -1.6650],
          [-1.8044, -0.7587, -0.1661,  ..., -1.6824,  0.6182,  0.9668],
          [-0.9504, -0.8981, -1.8044,  ..., -0.9330,  0.2348, -1.5430]]],


        [[[-2.1179, -0.7479, -2.1179,  ...,  0.2111,  0.7248, -1.1760],
          [-0.9363, -0.0972, -1.0048,  ..., -1.6384, -0.6109, -1.2274],
          [-1.5357, -2.0665, -0.7993,  ..., -1.6384, -0.1486, -0.3712],
          ...,
          [-0.7993,  0.3652, -0.8164,  ..., -1.2445, -0.8507, -0.8678],
          [-1.1247, -0.0801, -0.0287,  ..., -0.7137, -0.2856, -0.2171],
          [-1.5357, -1.4329, -2.1179,  ..., -1.7583, -2.1179, -1.8097]],

         [[-0.4251,  1.7983, -0.6877,  ...,  1.5182,  2.4286,  1.1506],
          [ 1.3782,  2.0959,  0.9055,  ..., -0.2325,  1.4482,  1.1681],
          [-0.0049, -0.6352,  0.4678,  ...,  0.0826,  2.1134,  2.1310],
          ...,
          [ 1.3256,  2.1310,  0.2402,  ..., -1.1253, -0.9153, -1.0028],
          [ 1.0805,  1.9559,  1.6057,  ...,  0.7654,  1.2381,  1.3081],
          [ 0.7304,  0.7129, -0.8277,  ...,  0.3978, -0.1800,  0.5553]],

         [[-1.8044,  0.5659, -1.0376,  ...,  2.0300,  1.8208, -0.4973],
          [ 0.1302,  1.2980,  1.0539,  ...,  0.0953,  0.4788, -0.4275],
          [-0.4798, -0.5844,  1.5942,  ..., -0.0092,  1.0539,  0.6008],
          ...,
          [ 1.2805,  2.0125, -0.0964,  ..., -1.1770,  0.5485,  1.2108],
          [ 0.8099,  1.8383,  1.8034,  ...,  1.6117,  2.0648,  2.0997],
          [ 0.3219,  0.6008, -0.3404,  ...,  1.6988,  0.3045,  0.6356]]]],
       device='cuda:0')
<class 'torch.Tensor'>
Traceback (most recent call last):
  File "/home/bscho/works/multi-tta/main.py", line 34, in <module>
    main()
  File "/home/bscho/works/multi-tta/main.py", line 24, in main
    trainer.train()
  File "/home/bscho/works/multi-tta/trainers/base_trainer.py", line 48, in train
    model = self.train_step(model, dataloader, optimizer)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/works/multi-tta/trainers/test_trainer.py", line 35, in train_step
    pred, uncertainty = self.mc_dropout(model, samples)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/works/multi-tta/trainers/test_trainer.py", line 99, in mc_dropout
    pred = model(samples)
           ^^^^^^^^^^^^^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/works/multi-tta/models/baseline.py", line 72, in forward
    x = self.patch_embed(samples)  # 패치 임베딩
        ^^^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
                      ^^^^^^^^^^^^^
bdb.BdbQuit
