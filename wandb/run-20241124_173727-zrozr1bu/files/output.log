  0%|                                                                                                              | 0/3125 [01:02<?, ?it/s]
> /home/bscho/works/multi-tta/trainers/test_trainer.py(39)train_step()
-> loss = softmax_entropy(pred['logits']).mean(0)
tensor([-8.1179e-08,  9.7840e-09,  1.5293e-08,  2.1167e-08, -4.4680e-08,
        -1.5352e-09, -5.7101e-09,  7.6465e-10,  2.0891e-08,  2.0345e-08,
         1.1692e-08,  1.7254e-08,  7.9231e-09,  6.0048e-08, -7.5144e-08,
        -9.7643e-09], device='cuda:0', grad_fn=<MeanBackward1>)
tensor(6.0048e-08, device='cuda:0', grad_fn=<UnbindBackward0>)
tensor(-8.1179e-08, device='cuda:0', grad_fn=<UnbindBackward0>)
Traceback (most recent call last):
  File "/home/bscho/works/multi-tta/main.py", line 34, in <module>
    main()
  File "/home/bscho/works/multi-tta/main.py", line 24, in main
    trainer.train()
  File "/home/bscho/works/multi-tta/trainers/base_trainer.py", line 48, in train
    model = self.train_step(model, dataloader, optimizer)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/works/multi-tta/trainers/test_trainer.py", line 39, in train_step
    loss = softmax_entropy(pred['logits']).mean(0)
           ^^^^^^^^^^^^^^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
                      ^^^^^^^^^^^^^
bdb.BdbQuit
