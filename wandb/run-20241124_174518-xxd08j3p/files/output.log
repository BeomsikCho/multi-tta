  0%|                                                                                                              | 0/3125 [00:03<?, ?it/s]
Traceback (most recent call last):
  File "/home/bscho/works/multi-tta/main.py", line 34, in <module>
    main()
  File "/home/bscho/works/multi-tta/main.py", line 24, in main
    trainer.train()
  File "/home/bscho/works/multi-tta/trainers/base_trainer.py", line 48, in train
    model = self.train_step(model, dataloader, optimizer)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/works/multi-tta/trainers/test_trainer.py", line 35, in train_step
    pred, uncertainty = self.mc_dropout(model, samples)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/works/multi-tta/trainers/test_trainer.py", line 79, in mc_dropout
    pred = model(samples)
           ^^^^^^^^^^^^^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 183, in forward
    inputs, module_kwargs = self.scatter(inputs, kwargs, self.device_ids)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py", line 207, in scatter
    return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/site-packages/torch/nn/parallel/scatter_gather.py", line 88, in scatter_kwargs
    scattered_inputs = scatter(inputs, target_gpus, dim) if inputs else []
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/site-packages/torch/nn/parallel/scatter_gather.py", line 75, in scatter
    res = scatter_map(inputs)
          ^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/site-packages/torch/nn/parallel/scatter_gather.py", line 62, in scatter_map
    return list(zip(*map(scatter_map, obj)))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/site-packages/torch/nn/parallel/scatter_gather.py", line 58, in scatter_map
    return Scatter.apply(target_gpus, None, dim, obj)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/site-packages/torch/nn/parallel/_functions.py", line 104, in forward
    outputs = comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/site-packages/torch/nn/parallel/comm.py", line 205, in scatter
    return tuple(torch._C._scatter(tensor, devices, chunk_sizes, dim, streams))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
