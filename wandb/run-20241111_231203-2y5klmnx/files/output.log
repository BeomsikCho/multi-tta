  0%|                                                                                                                                                                                                                                                             | 0/118 [00:00<?, ?it/s]
> /home/bscho/works/multi-tta/trainers/tent_trainer.py(32)train_step()
-> loss = softmax_entropy(pred['logits']).mean(0)
tensor([[[-2.0374, -1.0932, -0.3912,  ...,  0.8514, -1.1081, -1.6836],
         [-1.1542, -1.6992, -0.6898,  ...,  0.2332, -1.9821, -2.1766],
         [-1.6232, -0.9198, -0.2418,  ...,  0.8742, -0.7406, -1.9278],
         ...,
         [-1.0361, -1.6544,  0.1749,  ...,  1.2704, -0.6367, -1.6524],
         [-0.9843, -1.6746,  0.3647,  ...,  1.2577, -0.5333, -1.7964],
         [-1.2026, -1.4883,  0.2714,  ...,  0.9607, -0.5357, -1.9061]],

        [[-0.7563, -3.5356,  1.8183,  ..., -0.4609,  1.8962,  1.0687],
         [-0.6233, -3.3054,  1.7252,  ..., -0.2628,  1.8437,  1.0514],
         [-0.7798, -3.4875,  1.6615,  ..., -0.6304,  1.7567,  1.2186],
         ...,
         [ 2.7781, -0.9125,  0.9754,  ...,  0.2976,  1.6595,  1.7712],
         [ 2.7218, -1.9355,  0.8781,  ...,  0.3574,  1.8036,  1.2932],
         [ 0.9923, -1.0341,  0.7029,  ...,  0.7120,  1.1917,  1.7888]],

        [[ 0.7949, -0.8874, -1.0742,  ...,  1.5146,  2.5253,  0.5088],
         [-0.3505, -1.1093,  0.4321,  ...,  0.5966,  1.2469,  1.1676],
         [ 0.2998, -1.4344,  1.0580,  ...,  0.4881,  1.2632,  0.1702],
         ...,
         [-0.2558,  0.3377, -0.4054,  ...,  0.5779,  1.6157,  0.8439],
         [ 0.1354, -0.1647,  0.8797,  ...,  1.3951,  1.9976,  0.8003],
         [ 0.4868, -0.5753, -0.0599,  ...,  0.3050,  1.3926, -0.0153]],

        ...,

        [[-2.4697, -1.7917,  0.4191,  ...,  0.5389, -0.4417, -1.6040],
         [-1.8335, -1.8237,  0.2390,  ...,  0.9750, -0.1356, -1.5428],
         [-2.9973, -1.6166,  0.6113,  ...,  0.3329, -1.3511, -1.6294],
         ...,
         [-2.2178, -1.8161,  0.5889,  ...,  0.5411, -0.8673, -1.4907],
         [-1.5757, -1.7146,  0.6628,  ..., -0.9784, -1.8572, -0.0633],
         [-1.8628, -1.4983,  1.0535,  ...,  0.4085, -1.0449, -0.3160]],

        [[-1.3686, -1.2600, -0.1874,  ...,  0.0987,  1.3611, -0.2306],
         [-1.1375, -0.9036, -0.2278,  ..., -0.0190,  1.3450, -0.4264],
         [ 0.0212, -1.3071, -1.4588,  ...,  0.6279,  1.9987, -1.1977],
         ...,
         [-0.3969, -1.0302, -1.5220,  ...,  1.5965,  0.7881, -0.4141],
         [-1.2385, -1.6067, -0.5459,  ..., -0.3127,  1.5681,  0.0122],
         [ 0.1849, -1.8064, -1.2597,  ...,  0.5305,  2.3568, -0.3687]],

        [[-0.5325, -0.0600,  0.1793,  ..., -2.3377, -0.7011,  1.3023],
         [-0.4761, -0.0897,  0.0619,  ..., -1.9937, -0.6449,  1.0646],
         [-0.7387, -0.3868, -0.1066,  ..., -1.9235, -0.8839,  1.2215],
         ...,
         [-0.0391,  1.1220,  0.4691,  ..., -1.3337, -0.5231,  1.3586],
         [-0.8885, -0.6907,  0.9788,  ..., -2.2568, -0.2459,  0.7959],
         [-0.5401, -0.7861,  1.2805,  ..., -2.1906, -0.0157,  0.5327]]],
       device='cuda:0', grad_fn=<GatherBackward>)
torch.Size([64, 196, 1000])
torch.Size([64, 196, 1000])
Traceback (most recent call last):
  File "/home/bscho/works/multi-tta/main.py", line 34, in <module>
    main()
  File "/home/bscho/works/multi-tta/main.py", line 24, in main
    trainer.train()
  File "/home/bscho/works/multi-tta/trainers/base_trainer.py", line 48, in train
    model = self.train_step(model, dataloader, optimizer)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/works/multi-tta/trainers/tent_trainer.py", line 32, in train_step
    loss = softmax_entropy(pred['logits']).mean(0)
           ^^^^^^^^^^^^^^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
                      ^^^^^^^^^^^^^
bdb.BdbQuit
