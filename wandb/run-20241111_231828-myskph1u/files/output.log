  0%|                                                                                                                                                                                                                                                             | 0/118 [00:00<?, ?it/s]
> /home/bscho/works/multi-tta/trainers/tent_trainer.py(32)train_step()
-> loss = softmax_entropy(pred['logits']).mean(0)
{'last_hidden_state': tensor([[[ 5.6369e+00, -3.2311e+00, -4.4865e-01,  ...,  1.7768e+00,
          -1.7273e+00, -1.0813e+00],
         [ 5.1023e+00, -2.8200e+00, -1.6513e+00,  ...,  1.7445e+00,
          -1.6357e+00, -1.9369e+00],
         [ 4.9677e+00, -4.7794e+00, -1.0330e-01,  ...,  2.5068e+00,
          -1.7738e+00, -9.8629e-01],
         ...,
         [ 5.3816e+00, -2.6976e+00,  5.0618e-01,  ...,  2.6410e+00,
          -2.1389e+00, -1.0617e+00],
         [ 5.3752e+00, -2.5360e+00,  5.4148e-01,  ...,  2.1819e+00,
          -2.1616e+00, -7.0785e-01],
         [ 5.4933e+00, -2.3352e+00,  9.3699e-01,  ...,  2.0150e+00,
          -2.0043e+00, -1.4586e-01]],

        [[-1.5196e+00,  4.7002e-01, -1.3411e-01,  ..., -2.4706e+00,
           1.8794e-01, -4.9006e-01],
         [-1.3561e+00,  5.0220e-01, -6.4342e-01,  ..., -2.3730e+00,
           1.5720e-01, -4.9973e-01],
         [-1.6712e+00,  7.1601e-01,  9.3730e-02,  ..., -2.0660e+00,
           1.7975e-01, -6.9305e-01],
         ...,
         [-5.3321e-01,  1.2513e+00,  8.8354e-01,  ..., -8.1771e-01,
          -1.8964e+00, -5.5279e-02],
         [-1.1332e+00,  1.0561e+00,  1.0859e+00,  ...,  3.1037e-01,
          -9.7988e-01,  1.3657e+00],
         [ 6.3211e-01,  1.5838e+00,  2.1705e+00,  ...,  8.7235e-02,
          -2.4628e+00, -4.1140e-02]],

        [[-1.3638e-02, -1.4886e-01, -1.2623e+00,  ...,  1.3807e+00,
          -3.3846e-01,  9.1510e-01],
         [ 1.8530e+00,  1.6199e+00, -2.2126e+00,  ..., -3.5130e+00,
           1.6949e+00,  9.1401e-01],
         [ 2.7240e+00,  5.9208e-01, -1.7610e+00,  ..., -2.5366e+00,
          -1.0968e+00, -8.3010e-01],
         ...,
         [ 1.3264e-01,  1.4858e+00,  2.0641e+00,  ...,  5.0520e-01,
          -3.6838e-01,  1.5293e+00],
         [ 4.4916e+00, -9.3695e-01, -4.7396e-01,  ..., -3.3143e+00,
           1.4589e+00,  2.6584e-01],
         [ 2.7185e+00, -1.0969e+00, -1.9637e+00,  ..., -2.4580e+00,
          -1.2185e-03,  4.7002e-01]],

        ...,

        [[ 4.0151e+00, -3.1743e+00, -3.6718e+00,  ..., -1.6528e+00,
           6.7687e-01, -1.3479e+00],
         [ 2.2594e+00, -2.7817e+00, -2.4447e+00,  ..., -1.2765e+00,
           1.2125e+00,  5.9039e-01],
         [ 3.6706e+00, -2.9496e+00, -3.3117e+00,  ..., -3.6046e+00,
          -7.7961e-01, -1.0715e+00],
         ...,
         [ 2.6621e+00, -4.3664e+00, -1.2260e+00,  ..., -1.9766e+00,
           5.7851e-01, -1.2410e+00],
         [ 1.5069e+00, -6.9417e-01, -2.2443e+00,  ...,  3.3275e-02,
           4.5461e-01, -9.1207e-01],
         [ 2.8729e+00, -2.7224e+00, -4.5278e-01,  ..., -3.3101e+00,
          -1.1429e+00, -2.3384e+00]],

        [[ 4.0071e+00, -2.3183e+00,  1.5713e+00,  ...,  2.3020e+00,
           1.6713e-01, -2.6288e+00],
         [ 4.3429e+00, -1.7752e+00,  2.5499e+00,  ...,  2.1168e+00,
          -1.7342e-01, -2.6129e+00],
         [ 6.3089e-01, -6.2570e-01, -1.7627e-01,  ...,  7.5651e-01,
           1.6818e+00, -9.2534e-01],
         ...,
         [ 2.2502e+00, -1.1778e+00, -1.3193e+00,  ...,  3.3283e+00,
           8.1228e-01, -2.8384e-01],
         [ 4.4484e+00, -2.0546e+00,  7.0519e-01,  ...,  2.6497e+00,
          -5.2099e-02, -1.3120e+00],
         [ 1.3123e+00, -1.8200e+00,  1.1902e+00,  ...,  3.5411e+00,
          -2.1229e-01,  1.9645e+00]],

        [[-6.2494e-01,  2.1441e+00,  1.2304e+00,  ...,  5.5216e-01,
          -2.2609e+00,  4.3666e-01],
         [-6.8237e-01,  3.7198e+00,  1.3780e+00,  ..., -1.1667e-02,
          -2.3017e+00,  1.4542e+00],
         [-6.4020e-02,  3.3781e+00,  8.0575e-01,  ...,  1.5809e-01,
          -1.5143e+00,  7.6964e-01],
         ...,
         [-3.1361e+00,  4.2414e+00,  8.1289e-01,  ..., -1.0234e+00,
           2.3655e-01,  1.5147e-01],
         [-8.9343e-01,  5.8563e+00, -2.1883e-01,  ...,  6.7479e-01,
          -5.0807e-01, -1.2527e+00],
         [-3.4399e+00,  3.0586e+00,  9.4678e-01,  ...,  3.0170e+00,
          -8.4273e-01,  1.5062e+00]]], device='cuda:0',
       grad_fn=<GatherBackward>), 'logits': tensor([[-1.4948, -1.0106,  0.0847,  ...,  0.7242, -0.8886, -1.7969],
        [ 0.4382, -2.4165,  1.1638,  ..., -0.5380,  1.8913,  1.3562],
        [ 0.3286, -0.2892,  0.1087,  ...,  1.2809,  2.2376,  0.6108],
        ...,
        [-1.4328, -1.8085,  0.5537,  ..., -0.0614, -0.8129, -1.0304],
        [-0.8390, -0.8170, -0.9326,  ...,  0.4304,  1.4507, -0.6156],
        [-0.4860, -0.6237,  0.7845,  ..., -1.3069, -0.2170,  1.5261]],
       device='cuda:0', grad_fn=<GatherBackward>)}
torch.Size([64, 1000])
Traceback (most recent call last):
  File "/home/bscho/works/multi-tta/main.py", line 34, in <module>
    main()
  File "/home/bscho/works/multi-tta/main.py", line 24, in main
    trainer.train()
  File "/home/bscho/works/multi-tta/trainers/base_trainer.py", line 48, in train
    model = self.train_step(model, dataloader, optimizer)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/works/multi-tta/trainers/tent_trainer.py", line 32, in train_step
    loss = softmax_entropy(pred['logits']).mean(0)
           ^^^^^^^^^^^^^^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
                      ^^^^^^^^^^^^^
bdb.BdbQuit
