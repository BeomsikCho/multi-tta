> /home/bscho/works/multi-tta/trainers/test_trainer.py(60)set_dropout_layer()
-> if nm == 'attn' or nm == 'mlp':
''
> /home/bscho/works/multi-tta/trainers/test_trainer.py(58)set_dropout_layer()
-> for nm, m in model.named_modules():
''
> /home/bscho/works/multi-tta/trainers/test_trainer.py(59)set_dropout_layer()
-> breakpoint()
'module'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(60)set_dropout_layer()
-> if nm == 'attn' or nm == 'mlp':
'module'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(58)set_dropout_layer()
-> for nm, m in model.named_modules():
'module'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(59)set_dropout_layer()
-> breakpoint()
'module.model'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(60)set_dropout_layer()
-> if nm == 'attn' or nm == 'mlp':
'module.model'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(58)set_dropout_layer()
-> for nm, m in model.named_modules():
'module.model'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(59)set_dropout_layer()
-> breakpoint()
'module.model.patch_embed'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(60)set_dropout_layer()
-> if nm == 'attn' or nm == 'mlp':
'module.model.patch_embed'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(58)set_dropout_layer()
-> for nm, m in model.named_modules():
'module.model.patch_embed'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(59)set_dropout_layer()
-> breakpoint()
'module.model.patch_embed.proj'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(60)set_dropout_layer()
-> if nm == 'attn' or nm == 'mlp':
'module.model.patch_embed.proj'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(58)set_dropout_layer()
-> for nm, m in model.named_modules():
'module.model.patch_embed.proj'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(59)set_dropout_layer()
-> breakpoint()
'module.model.patch_embed.norm'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(60)set_dropout_layer()
-> if nm == 'attn' or nm == 'mlp':
'module.model.patch_embed.norm'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(58)set_dropout_layer()
-> for nm, m in model.named_modules():
'module.model.patch_embed.norm'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(59)set_dropout_layer()
-> breakpoint()
'module.model.pos_drop'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(60)set_dropout_layer()
-> if nm == 'attn' or nm == 'mlp':
'module.model.pos_drop'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(58)set_dropout_layer()
-> for nm, m in model.named_modules():
'module.model.pos_drop'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(59)set_dropout_layer()
-> breakpoint()
'module.model.patch_drop'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(60)set_dropout_layer()
-> if nm == 'attn' or nm == 'mlp':
'module.model.patch_drop'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(58)set_dropout_layer()
-> for nm, m in model.named_modules():
'module.model.patch_drop'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(59)set_dropout_layer()
-> breakpoint()
'module.model.norm_pre'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(60)set_dropout_layer()
-> if nm == 'attn' or nm == 'mlp':
'module.model.norm_pre'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(58)set_dropout_layer()
-> for nm, m in model.named_modules():
'module.model.norm_pre'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(59)set_dropout_layer()
-> breakpoint()
'module.model.blocks'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(60)set_dropout_layer()
-> if nm == 'attn' or nm == 'mlp':
'module.model.blocks'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(58)set_dropout_layer()
-> for nm, m in model.named_modules():
'module.model.blocks'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(59)set_dropout_layer()
-> breakpoint()
'module.model.blocks.0'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(60)set_dropout_layer()
-> if nm == 'attn' or nm == 'mlp':
'module.model.blocks.0'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(58)set_dropout_layer()
-> for nm, m in model.named_modules():
> /home/bscho/works/multi-tta/trainers/test_trainer.py(59)set_dropout_layer()
-> breakpoint()
'module.model.blocks.0.norm1'
> /home/bscho/works/multi-tta/trainers/test_trainer.py(60)set_dropout_layer()
-> if nm == 'attn' or nm == 'mlp':
'module.model.blocks.0.norm1'
Traceback (most recent call last):
  File "/home/bscho/works/multi-tta/main.py", line 34, in <module>
    main()
  File "/home/bscho/works/multi-tta/main.py", line 24, in main
    trainer.train()
  File "/home/bscho/works/multi-tta/trainers/base_trainer.py", line 48, in train
    model = self.train_step(model, dataloader, optimizer)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/works/multi-tta/trainers/test_trainer.py", line 20, in train_step
    model = self.set_dropout_layer(model) # 수정한 내용!!!!!!!!!
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/works/multi-tta/trainers/test_trainer.py", line 60, in set_dropout_layer
    if nm == 'attn' or nm == 'mlp':
       ^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bscho/anaconda3/envs/multiTTA/lib/python3.12/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
                      ^^^^^^^^^^^^^
bdb.BdbQuit
